{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Dropout, AlphaDropout, Activation, BatchNormalization, Flatten, \\\n",
    "                                    Concatenate, PReLU\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
    "\n",
    "sys.path.insert(0, \"../../python\")\n",
    "from common import *\n",
    "from DataLoader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSetup:\n",
    "    def __init__(self, activation, activation_shared_axes, dropout_rate, first_layer_size, last_layer_size, decay_factor,\n",
    "                 kernel_regularizer):\n",
    "        self.activation = activation\n",
    "        self.activation_shared_axes = activation_shared_axes\n",
    "        if activation == 'relu' or activation == 'PReLU':\n",
    "            self.DropoutType = Dropout\n",
    "            self.kernel_init = 'he_uniform'\n",
    "            self.apply_batch_norm = True\n",
    "        elif activation == 'selu':\n",
    "            self.DropoutType = AlphaDropout\n",
    "            self.kernel_init = 'lecun_normal'\n",
    "            self.apply_batch_norm = False\n",
    "        else:\n",
    "            raise RuntimeError('Activation \"{}\" not supported.'.format(activation))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.first_layer_size = first_layer_size\n",
    "        self.last_layer_size = last_layer_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "    \n",
    "    def RecalcLayerSizes(self, n_input_features, width_factor, compression_factor):\n",
    "        self.first_layer_size = int(math.ceil(n_input_features * (1 + self.dropout_rate) * width_factor))\n",
    "        self.last_layer_size = int(math.ceil(n_input_features * (1 + self.dropout_rate) * compression_factor))\n",
    "    \n",
    "def add_block_ending(net_setup, name_format, layer):\n",
    "    if net_setup.apply_batch_norm:\n",
    "        norm_layer = BatchNormalization(name=name_format.format('norm'))(layer)\n",
    "    else:\n",
    "        norm_layer = layer\n",
    "    if net_setup.activation == 'PReLU':\n",
    "        activation_layer = PReLU(shared_axes=net_setup.activation_shared_axes,\n",
    "                                 name=name_format.format('activation'))(norm_layer)\n",
    "    else:\n",
    "        activation_layer = Activation(net_setup.activation, name=name_format.format('activation'))(norm_layer)\n",
    "    if net_setup.dropout_rate > 0:\n",
    "        return net_setup.DropoutType(net_setup.dropout_rate, name=name_format.format('dropout'))(activation_layer)\n",
    "    return activation_layer\n",
    "\n",
    "def dense_block(prev_layer, kernel_size, net_setup, block_name, n):\n",
    "    dense = Dense(kernel_size, name=\"{}_dense_{}\".format(block_name, n),\n",
    "                  kernel_initializer=net_setup.kernel_init,\n",
    "                  kernel_regularizer=net_setup.kernel_regularizer)(prev_layer)\n",
    "    return add_block_ending(net_setup, '{}_{{}}_{}'.format(block_name, n), dense)\n",
    "\n",
    "def conv_block(prev_layer, filters, kernel_size, net_setup, block_name, n):\n",
    "    conv = Conv2D(filters, kernel_size, name=\"{}_conv_{}\".format(block_name, n),\n",
    "                  kernel_initializer=net_setup.kernel_init)(prev_layer)\n",
    "    return add_block_ending(net_setup, '{}_{{}}_{}'.format(block_name, n), conv)\n",
    "\n",
    "def reduce_n_features_1d(input_layer, net_setup, block_name):\n",
    "    prev_layer = input_layer\n",
    "    current_size = net_setup.first_layer_size\n",
    "    n = 1\n",
    "    while True:\n",
    "        prev_layer = dense_block(prev_layer, current_size, net_setup, block_name, n)\n",
    "        if current_size == net_setup.last_layer_size: break\n",
    "        current_size = max(net_setup.last_layer_size, int(current_size / net_setup.decay_factor))\n",
    "        n += 1\n",
    "    return prev_layer\n",
    "\n",
    "\n",
    "def reduce_n_features_2d(input_layer, net_setup, block_name):\n",
    "    conv_kernel=(1, 1)\n",
    "    prev_layer = input_layer\n",
    "    current_size = net_setup.first_layer_size\n",
    "    n = 1\n",
    "    while True:\n",
    "        prev_layer = conv_block(prev_layer, current_size, conv_kernel, net_setup, block_name, n)\n",
    "        if current_size == net_setup.last_layer_size: break\n",
    "        current_size = max(net_setup.last_layer_size, int(current_size / net_setup.decay_factor))\n",
    "        n += 1\n",
    "    return prev_layer\n",
    "\n",
    "def create_model(net_config):\n",
    "    tau_net_setup = NetSetup('PReLU', None, 0.25, 128, 128, 1.4, None)\n",
    "    comp_net_setup = NetSetup('PReLU', [1, 2], 0.25, 1024, 64, 1.4, None)\n",
    "    #dense_net_setup = NetSetup('relu', 0, 512, 32, 1.4, keras.regularizers.l1(1e-5))\n",
    "    dense_net_setup = NetSetup('PReLU', None, 0.25, 512, 64, 1.4, None)\n",
    "            \n",
    "    model_name = \"DeepTau2017v2p3\"\n",
    "    input_layers = []\n",
    "    high_level_features = []\n",
    "\n",
    "    if len(net_config.tau_branches) > 0:\n",
    "        input_layer_tau = Input(name=\"input_tau\", shape=(len(net_config.tau_branches),))\n",
    "        input_layers.append(input_layer_tau)\n",
    "        tau_net_setup.RecalcLayerSizes(len(net_config.tau_branches), 1.5, 1.5)\n",
    "        reduced_tau = reduce_n_features_1d(input_layer_tau, tau_net_setup, 'tau')\n",
    "        high_level_features = [ reduced_tau ]\n",
    "    \n",
    "    for loc in net_config.cell_locations:\n",
    "        reduced_inputs = []\n",
    "        for comp_id in range(len(net_config.comp_names)):\n",
    "            comp_name = net_config.comp_names[comp_id]\n",
    "            n_comp_features = len(input_cell_external_branches) + len(net_config.comp_branches[comp_id])\n",
    "            input_layer_comp = Input(name=\"input_{}_{}\".format(loc, comp_name),\n",
    "                                     shape=(n_cells_eta[loc] * n_cells_phi[loc], n_comp_features))\n",
    "            input_layers.append(input_layer_comp)\n",
    "            input_masked = Masking(name=\"masking_{}_{}\".format(loc, comp_name))(input_layer_comp)\n",
    "            comp_net_setup.RecalcLayerSizes(n_comp_features, 4, 0.8)\n",
    "            reduced_comp = reduce_n_features_2d(input_layer_comp, comp_net_setup, \"{}_{}\".format(loc, comp_name))\n",
    "            reduced_inputs.append(reduced_comp)\n",
    "            \n",
    "        cell_output_size = 64\n",
    "        if len(component_names) > 1:\n",
    "            conv_all_start = Concatenate(name=\"{}_cell_concat\".format(loc), axis=3)(reduced_inputs)\n",
    "            comp_net_setup.first_layer_size = 512\n",
    "            comp_net_setup.last_layer_size = 64\n",
    "            prev_layer = reduce_n_features_2d(conv_all_start, comp_net_setup, \"{}_all\".format(loc))\n",
    "        else:\n",
    "            prev_layer = reduced_inputs[0]\n",
    "        window_size = 2\n",
    "        current_size = n_cells_eta[loc]\n",
    "        n = 1\n",
    "        while current_size > 1:\n",
    "            win_size = min(current_size, window_size)\n",
    "            prev_layer = conv_block(prev_layer, cell_output_size, (win_size, win_size), comp_net_setup,\n",
    "                                    \"{}_all_{}x{}\".format(loc, win_size, win_size), n)\n",
    "            n += 1\n",
    "            current_size -= window_size - 1\n",
    "            \n",
    "        cells_flatten = Flatten(name=\"{}_cells_flatten\".format(loc))(prev_layer)\n",
    "        high_level_features.append(cells_flatten)\n",
    "        \n",
    "    if len(high_level_features) > 1:\n",
    "        features_concat = Concatenate(name=\"features_concat\")(high_level_features)\n",
    "    else:\n",
    "        features_concat = high_level_features[0]\n",
    "    if net_conf.final:\n",
    "        final_dense = reduce_n_features_1d(features_concat, dense_net_setup, 'final')\n",
    "    else:\n",
    "        final_delse = dense_block(features_concat, 2048, dense_net_setup, 'tmp', 1)\n",
    "\n",
    "    output_layer = Dense(n_outputs, name=\"final_dense_{}\".format(n),\n",
    "                         kernel_initializer=dense_net_setup.kernel_init)(final_dense)\n",
    "    softmax_output = Activation(\"softmax\", name=\"main_output\")(output_layer)\n",
    "\n",
    "    model = Model(input_layers, softmax_output, name=\"DeepTau2017v2\")\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate):\n",
    "    opt = keras.optimizers.Adam(lr=learning_rate)\n",
    "    #opt = keras.optimizers.Nadam(lr=learning_rate)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    metrics = [\"accuracy\", TauLosses.tau_crossentropy, TauLosses.Le, TauLosses.Lmu, TauLosses.Ljet, TauLosses.sLe, TauLosses.sLmu, TauLosses.sLjet ]\n",
    "    model.compile(loss=TauLosses.tau_crossentropy, optimizer=opt, metrics=metrics, weighted_metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_inner_cmb (InputLayer)    (None, 11, 11, 180)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_outer_cmb (InputLayer)    (None, 11, 11, 180)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_1 (Conv2D)       (None, 11, 11, 1024) 185344      input_inner_cmb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_1 (Conv2D)       (None, 11, 11, 1024) 185344      input_outer_cmb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_1 (BatchNormaliz (None, 11, 11, 1024) 4096        inner_cmb_conv_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_1 (BatchNormaliz (None, 11, 11, 1024) 4096        outer_cmb_conv_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_1 (PReLU)  (None, 11, 11, 1024) 1024        inner_cmb_norm_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_1 (PReLU)  (None, 11, 11, 1024) 1024        outer_cmb_norm_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_1 (Dropout)   (None, 11, 11, 1024) 0           inner_cmb_activation_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_1 (Dropout)   (None, 11, 11, 1024) 0           outer_cmb_activation_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_2 (Conv2D)       (None, 11, 11, 731)  749275      inner_cmb_dropout_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_2 (Conv2D)       (None, 11, 11, 731)  749275      outer_cmb_dropout_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_2 (BatchNormaliz (None, 11, 11, 731)  2924        inner_cmb_conv_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_2 (BatchNormaliz (None, 11, 11, 731)  2924        outer_cmb_conv_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_2 (PReLU)  (None, 11, 11, 731)  731         inner_cmb_norm_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_2 (PReLU)  (None, 11, 11, 731)  731         outer_cmb_norm_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_2 (Dropout)   (None, 11, 11, 731)  0           inner_cmb_activation_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_2 (Dropout)   (None, 11, 11, 731)  0           outer_cmb_activation_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_3 (Conv2D)       (None, 11, 11, 522)  382104      inner_cmb_dropout_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_3 (Conv2D)       (None, 11, 11, 522)  382104      outer_cmb_dropout_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_3 (BatchNormaliz (None, 11, 11, 522)  2088        inner_cmb_conv_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_3 (BatchNormaliz (None, 11, 11, 522)  2088        outer_cmb_conv_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_3 (PReLU)  (None, 11, 11, 522)  522         inner_cmb_norm_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_3 (PReLU)  (None, 11, 11, 522)  522         outer_cmb_norm_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_3 (Dropout)   (None, 11, 11, 522)  0           inner_cmb_activation_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_3 (Dropout)   (None, 11, 11, 522)  0           outer_cmb_activation_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_4 (Conv2D)       (None, 11, 11, 372)  194556      inner_cmb_dropout_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_4 (Conv2D)       (None, 11, 11, 372)  194556      outer_cmb_dropout_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_4 (BatchNormaliz (None, 11, 11, 372)  1488        inner_cmb_conv_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_4 (BatchNormaliz (None, 11, 11, 372)  1488        outer_cmb_conv_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_4 (PReLU)  (None, 11, 11, 372)  372         inner_cmb_norm_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_4 (PReLU)  (None, 11, 11, 372)  372         outer_cmb_norm_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_4 (Dropout)   (None, 11, 11, 372)  0           inner_cmb_activation_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_4 (Dropout)   (None, 11, 11, 372)  0           outer_cmb_activation_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_5 (Conv2D)       (None, 11, 11, 265)  98845       inner_cmb_dropout_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_5 (Conv2D)       (None, 11, 11, 265)  98845       outer_cmb_dropout_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_5 (BatchNormaliz (None, 11, 11, 265)  1060        inner_cmb_conv_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_5 (BatchNormaliz (None, 11, 11, 265)  1060        outer_cmb_conv_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_5 (PReLU)  (None, 11, 11, 265)  265         inner_cmb_norm_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_5 (PReLU)  (None, 11, 11, 265)  265         outer_cmb_norm_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_5 (Dropout)   (None, 11, 11, 265)  0           inner_cmb_activation_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_5 (Dropout)   (None, 11, 11, 265)  0           outer_cmb_activation_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_6 (Conv2D)       (None, 11, 11, 189)  50274       inner_cmb_dropout_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_6 (Conv2D)       (None, 11, 11, 189)  50274       outer_cmb_dropout_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_6 (BatchNormaliz (None, 11, 11, 189)  756         inner_cmb_conv_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_6 (BatchNormaliz (None, 11, 11, 189)  756         outer_cmb_conv_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_6 (PReLU)  (None, 11, 11, 189)  189         inner_cmb_norm_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_6 (PReLU)  (None, 11, 11, 189)  189         outer_cmb_norm_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_6 (Dropout)   (None, 11, 11, 189)  0           inner_cmb_activation_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_6 (Dropout)   (None, 11, 11, 189)  0           outer_cmb_activation_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_7 (Conv2D)       (None, 11, 11, 135)  25650       inner_cmb_dropout_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_7 (Conv2D)       (None, 11, 11, 135)  25650       outer_cmb_dropout_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_7 (BatchNormaliz (None, 11, 11, 135)  540         inner_cmb_conv_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_7 (BatchNormaliz (None, 11, 11, 135)  540         outer_cmb_conv_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_7 (PReLU)  (None, 11, 11, 135)  135         inner_cmb_norm_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_7 (PReLU)  (None, 11, 11, 135)  135         outer_cmb_norm_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_7 (Dropout)   (None, 11, 11, 135)  0           inner_cmb_activation_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_7 (Dropout)   (None, 11, 11, 135)  0           outer_cmb_activation_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_8 (Conv2D)       (None, 11, 11, 96)   13056       inner_cmb_dropout_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_8 (Conv2D)       (None, 11, 11, 96)   13056       outer_cmb_dropout_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_8 (BatchNormaliz (None, 11, 11, 96)   384         inner_cmb_conv_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_8 (BatchNormaliz (None, 11, 11, 96)   384         outer_cmb_conv_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_8 (PReLU)  (None, 11, 11, 96)   96          inner_cmb_norm_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_8 (PReLU)  (None, 11, 11, 96)   96          outer_cmb_norm_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_8 (Dropout)   (None, 11, 11, 96)   0           inner_cmb_activation_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_8 (Dropout)   (None, 11, 11, 96)   0           outer_cmb_activation_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_9 (Conv2D)       (None, 11, 11, 68)   6596        inner_cmb_dropout_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_9 (Conv2D)       (None, 11, 11, 68)   6596        outer_cmb_dropout_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_9 (BatchNormaliz (None, 11, 11, 68)   272         inner_cmb_conv_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_9 (BatchNormaliz (None, 11, 11, 68)   272         outer_cmb_conv_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_9 (PReLU)  (None, 11, 11, 68)   68          inner_cmb_norm_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_9 (PReLU)  (None, 11, 11, 68)   68          outer_cmb_norm_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_9 (Dropout)   (None, 11, 11, 68)   0           inner_cmb_activation_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_9 (Dropout)   (None, 11, 11, 68)   0           outer_cmb_activation_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_conv_10 (Conv2D)      (None, 11, 11, 64)   4416        inner_cmb_dropout_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_conv_10 (Conv2D)      (None, 11, 11, 64)   4416        outer_cmb_dropout_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_norm_10 (BatchNormali (None, 11, 11, 64)   256         inner_cmb_conv_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_norm_10 (BatchNormali (None, 11, 11, 64)   256         outer_cmb_conv_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_activation_10 (PReLU) (None, 11, 11, 64)   64          inner_cmb_norm_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_activation_10 (PReLU) (None, 11, 11, 64)   64          outer_cmb_norm_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inner_cmb_dropout_10 (Dropout)  (None, 11, 11, 64)   0           inner_cmb_activation_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_cmb_dropout_10 (Dropout)  (None, 11, 11, 64)   0           outer_cmb_activation_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_1 (Conv2D)   (None, 10, 10, 64)   16448       inner_cmb_dropout_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_1 (Conv2D)   (None, 10, 10, 64)   16448       outer_cmb_dropout_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_1 (BatchNorm (None, 10, 10, 64)   256         inner_all_2x2_conv_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_1 (BatchNorm (None, 10, 10, 64)   256         outer_all_2x2_conv_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_1 (PRe (None, 10, 10, 64)   64          inner_all_2x2_norm_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_1 (PRe (None, 10, 10, 64)   64          outer_all_2x2_norm_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_1 (Dropou (None, 10, 10, 64)   0           inner_all_2x2_activation_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_1 (Dropou (None, 10, 10, 64)   0           outer_all_2x2_activation_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_2 (Conv2D)   (None, 9, 9, 64)     16448       inner_all_2x2_dropout_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_2 (Conv2D)   (None, 9, 9, 64)     16448       outer_all_2x2_dropout_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_2 (BatchNorm (None, 9, 9, 64)     256         inner_all_2x2_conv_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_2 (BatchNorm (None, 9, 9, 64)     256         outer_all_2x2_conv_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_2 (PRe (None, 9, 9, 64)     64          inner_all_2x2_norm_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_2 (PRe (None, 9, 9, 64)     64          outer_all_2x2_norm_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_2 (Dropou (None, 9, 9, 64)     0           inner_all_2x2_activation_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_2 (Dropou (None, 9, 9, 64)     0           outer_all_2x2_activation_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_3 (Conv2D)   (None, 8, 8, 64)     16448       inner_all_2x2_dropout_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_3 (Conv2D)   (None, 8, 8, 64)     16448       outer_all_2x2_dropout_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_3 (BatchNorm (None, 8, 8, 64)     256         inner_all_2x2_conv_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_3 (BatchNorm (None, 8, 8, 64)     256         outer_all_2x2_conv_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_3 (PRe (None, 8, 8, 64)     64          inner_all_2x2_norm_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_3 (PRe (None, 8, 8, 64)     64          outer_all_2x2_norm_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_3 (Dropou (None, 8, 8, 64)     0           inner_all_2x2_activation_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_3 (Dropou (None, 8, 8, 64)     0           outer_all_2x2_activation_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_4 (Conv2D)   (None, 7, 7, 64)     16448       inner_all_2x2_dropout_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_4 (Conv2D)   (None, 7, 7, 64)     16448       outer_all_2x2_dropout_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_4 (BatchNorm (None, 7, 7, 64)     256         inner_all_2x2_conv_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_4 (BatchNorm (None, 7, 7, 64)     256         outer_all_2x2_conv_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_4 (PRe (None, 7, 7, 64)     64          inner_all_2x2_norm_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_4 (PRe (None, 7, 7, 64)     64          outer_all_2x2_norm_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_4 (Dropou (None, 7, 7, 64)     0           inner_all_2x2_activation_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_4 (Dropou (None, 7, 7, 64)     0           outer_all_2x2_activation_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_5 (Conv2D)   (None, 6, 6, 64)     16448       inner_all_2x2_dropout_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_5 (Conv2D)   (None, 6, 6, 64)     16448       outer_all_2x2_dropout_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_5 (BatchNorm (None, 6, 6, 64)     256         inner_all_2x2_conv_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_5 (BatchNorm (None, 6, 6, 64)     256         outer_all_2x2_conv_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_5 (PRe (None, 6, 6, 64)     64          inner_all_2x2_norm_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_5 (PRe (None, 6, 6, 64)     64          outer_all_2x2_norm_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_5 (Dropou (None, 6, 6, 64)     0           inner_all_2x2_activation_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_5 (Dropou (None, 6, 6, 64)     0           outer_all_2x2_activation_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_6 (Conv2D)   (None, 5, 5, 64)     16448       inner_all_2x2_dropout_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_6 (Conv2D)   (None, 5, 5, 64)     16448       outer_all_2x2_dropout_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_6 (BatchNorm (None, 5, 5, 64)     256         inner_all_2x2_conv_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_6 (BatchNorm (None, 5, 5, 64)     256         outer_all_2x2_conv_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_6 (PRe (None, 5, 5, 64)     64          inner_all_2x2_norm_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_6 (PRe (None, 5, 5, 64)     64          outer_all_2x2_norm_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_6 (Dropou (None, 5, 5, 64)     0           inner_all_2x2_activation_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_6 (Dropou (None, 5, 5, 64)     0           outer_all_2x2_activation_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_7 (Conv2D)   (None, 4, 4, 64)     16448       inner_all_2x2_dropout_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_7 (Conv2D)   (None, 4, 4, 64)     16448       outer_all_2x2_dropout_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_7 (BatchNorm (None, 4, 4, 64)     256         inner_all_2x2_conv_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_7 (BatchNorm (None, 4, 4, 64)     256         outer_all_2x2_conv_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_7 (PRe (None, 4, 4, 64)     64          inner_all_2x2_norm_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_7 (PRe (None, 4, 4, 64)     64          outer_all_2x2_norm_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_7 (Dropou (None, 4, 4, 64)     0           inner_all_2x2_activation_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_7 (Dropou (None, 4, 4, 64)     0           outer_all_2x2_activation_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_8 (Conv2D)   (None, 3, 3, 64)     16448       inner_all_2x2_dropout_7[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_8 (Conv2D)   (None, 3, 3, 64)     16448       outer_all_2x2_dropout_7[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_8 (BatchNorm (None, 3, 3, 64)     256         inner_all_2x2_conv_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_8 (BatchNorm (None, 3, 3, 64)     256         outer_all_2x2_conv_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_8 (PRe (None, 3, 3, 64)     64          inner_all_2x2_norm_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_8 (PRe (None, 3, 3, 64)     64          outer_all_2x2_norm_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_8 (Dropou (None, 3, 3, 64)     0           inner_all_2x2_activation_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_8 (Dropou (None, 3, 3, 64)     0           outer_all_2x2_activation_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_9 (Conv2D)   (None, 2, 2, 64)     16448       inner_all_2x2_dropout_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_9 (Conv2D)   (None, 2, 2, 64)     16448       outer_all_2x2_dropout_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_9 (BatchNorm (None, 2, 2, 64)     256         inner_all_2x2_conv_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_9 (BatchNorm (None, 2, 2, 64)     256         outer_all_2x2_conv_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_9 (PRe (None, 2, 2, 64)     64          inner_all_2x2_norm_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_9 (PRe (None, 2, 2, 64)     64          outer_all_2x2_norm_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_9 (Dropou (None, 2, 2, 64)     0           inner_all_2x2_activation_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_9 (Dropou (None, 2, 2, 64)     0           outer_all_2x2_activation_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_conv_10 (Conv2D)  (None, 1, 1, 64)     16448       inner_all_2x2_dropout_9[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_conv_10 (Conv2D)  (None, 1, 1, 64)     16448       outer_all_2x2_dropout_9[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_norm_10 (BatchNor (None, 1, 1, 64)     256         inner_all_2x2_conv_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_norm_10 (BatchNor (None, 1, 1, 64)     256         outer_all_2x2_conv_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_activation_10 (PR (None, 1, 1, 64)     64          inner_all_2x2_norm_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_activation_10 (PR (None, 1, 1, 64)     64          outer_all_2x2_norm_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "inner_all_2x2_dropout_10 (Dropo (None, 1, 1, 64)     0           inner_all_2x2_activation_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "outer_all_2x2_dropout_10 (Dropo (None, 1, 1, 64)     0           outer_all_2x2_activation_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "input_tau (InputLayer)          (None, 46)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inner_cells_flatten (Flatten)   (None, 64)           0           inner_all_2x2_dropout_10[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "outer_cells_flatten (Flatten)   (None, 64)           0           outer_all_2x2_dropout_10[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "features_concat (Concatenate)   (None, 174)          0           input_tau[0][0]                  \n",
      "                                                                 inner_cells_flatten[0][0]        \n",
      "                                                                 outer_cells_flatten[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_1 (Dense)           (None, 512)          89600       features_concat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_1 (BatchNormalizatio (None, 512)          2048        final_dense_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_1 (PReLU)      (None, 512)          512         final_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_1 (Dropout)       (None, 512)          0           final_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_2 (Dense)           (None, 365)          187245      final_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_2 (BatchNormalizatio (None, 365)          1460        final_dense_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_2 (PReLU)      (None, 365)          365         final_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_2 (Dropout)       (None, 365)          0           final_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_3 (Dense)           (None, 260)          95160       final_dropout_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_3 (BatchNormalizatio (None, 260)          1040        final_dense_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_3 (PReLU)      (None, 260)          260         final_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_3 (Dropout)       (None, 260)          0           final_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_4 (Dense)           (None, 185)          48285       final_dropout_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_4 (BatchNormalizatio (None, 185)          740         final_dense_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_4 (PReLU)      (None, 185)          185         final_norm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_4 (Dropout)       (None, 185)          0           final_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_5 (Dense)           (None, 132)          24552       final_dropout_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_5 (BatchNormalizatio (None, 132)          528         final_dense_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_5 (PReLU)      (None, 132)          132         final_norm_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_5 (Dropout)       (None, 132)          0           final_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_6 (Dense)           (None, 94)           12502       final_dropout_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_6 (BatchNormalizatio (None, 94)           376         final_dense_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_6 (PReLU)      (None, 94)           94          final_norm_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_6 (Dropout)       (None, 94)           0           final_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_7 (Dense)           (None, 67)           6365        final_dropout_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_7 (BatchNormalizatio (None, 67)           268         final_dense_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_7 (PReLU)      (None, 67)           67          final_norm_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_7 (Dropout)       (None, 67)           0           final_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_8 (Dense)           (None, 64)           4352        final_dropout_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_norm_8 (BatchNormalizatio (None, 64)           256         final_dense_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_activation_8 (PReLU)      (None, 64)           64          final_norm_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "final_dropout_8 (Dropout)       (None, 64)           0           final_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_9 (Dense)           (None, 4)            260         final_dropout_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Activation)        (None, 4)            0           final_dense_9[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,266,968\n",
      "Trainable params: 4,247,186\n",
      "Non-trainable params: 19,782\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "TauLosses.SetSFs(1, 1, 1)\n",
    "model, model_name = create_model()\n",
    "compile_model(model, 1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_file(f_name):\n",
    "    file_objs = [ obj for obj in gc.get_objects() if (\"TextIOWrapper\" in str(type(obj))) and (obj.name == f_name)]\n",
    "    for obj in file_objs:\n",
    "        obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCheckpoint(Callback):\n",
    "    def __init__(self, time_interval, file_name_prefix):\n",
    "        self.time_interval = time_interval\n",
    "        self.file_name_prefix = file_name_prefix\n",
    "        self.initial_time = time.time()\n",
    "        self.last_check_time = self.initial_time\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 100 != 0: return\n",
    "        current_time = time.time()\n",
    "        delta_t = current_time - self.last_check_time\n",
    "        if delta_t >= self.time_interval:\n",
    "            abs_delta_t_h = (current_time - self.initial_time) / 60. / 60.\n",
    "            self.model.save('{}_b{}_{:.1f}h.h5'.format(self.file_name_prefix, batch, abs_delta_t_h))\n",
    "            self.last_check_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_suffix, model_name, data_loader, epoch, n_epochs):\n",
    "\n",
    "    train_name = '%s_%s' % (model_name, train_suffix)\n",
    "    \n",
    "    cb_acc = []\n",
    "    for acc_name in [\"acc\", \"weighted_acc\"]:\n",
    "        cb_acc.append(ModelCheckpoint(\"%s_acc.hdf5\" % train_name, monitor=\"val_%s\" % acc_name, save_best_only=True,\n",
    "                                      save_weights_only=False, mode=\"max\", verbose=1))\n",
    "    \n",
    "    cb_losses = []\n",
    "    for loss_name in [\"loss\", \"tau_crossentropy\", \"Le\", \"Lmu\", \"Ljet\",\n",
    "                      \"weighted_tau_crossentropy\", \"weighted_Le\", \"weighted_Lmu\", \"weighted_Ljet\"]:\n",
    "        cb_losses.append(ModelCheckpoint(\"%s_%s.hdf5\" % (train_name, loss_name), monitor=\"val_%s\" % loss_name,\n",
    "                                         save_best_only=True, save_weights_only=False, mode=\"min\", verbose=1))\n",
    "\n",
    "    log_name = \"%s.log\" % train_name\n",
    "    if os.path.isfile(log_name):\n",
    "        close_file(log_name)\n",
    "        os.remove(log_name)\n",
    "    csv_log = CSVLogger(log_name, append=True)\n",
    "\n",
    "    time_checkpoint = TimeCheckpoint(4*60*60, '{}_historic'.format(train_name))\n",
    "    callbacks = [time_checkpoint, csv_log, *cb_acc, *cb_losses]\n",
    "    fit_hist = model.fit_generator(data_loader.generator(True), validation_data=data_loader.generator(False),\n",
    "                                   steps_per_epoch=data_loader.steps_per_epoch, validation_steps=data_loader.validation_steps,\n",
    "                                   callbacks=callbacks, epochs=n_epochs, initial_epoch=epoch, verbose=1)\n",
    "\n",
    "    model.save(\"%s_final.hdf5\" % train_name)\n",
    "    return fit_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72491602 66391602 6100000\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('N:/tau-ml/tuples-v2-t3/training/part_*.h5', 200, 10000, validation_size=6100000,\n",
    "                    max_queue_size=100, n_passes=-1)\n",
    "\n",
    "print(loader.total_size, loader.data_size, loader.validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "30500/30500 [==============================] - 4661s 153ms/step - loss: 0.1323 - acc: 0.9501 - tau_crossentropy: 0.1369 - Le: 0.0434 - Lmu: 0.0055 - Ljet: 0.0880 - sLe: 0.0434 - sLmu: 0.0055 - sLjet: 0.0880 - weighted_acc: 0.9498 - weighted_tau_crossentropy: 0.1354 - weighted_Le: 0.0557 - weighted_Lmu: 0.0065 - weighted_Ljet: 0.0732 - weighted_sLe: 0.0557 - weighted_sLmu: 0.0065 - weighted_sLjet: 0.073222:42 - loss: 0.1304 - acc: 0.9507 - tau_crossentropy: 0.1358 - Le: 0.0429 - Lmu: 0.0054 - Ljet: 0.0875 - sLe: 0.0429 - sLmu: 0.0054 - sLjet: 0.0875 - weighted_acc: 0.9513 - weighted_tau_crossentropy: 0.1325 - weighted_Le: 0.0555 - weighted_Lmu: 0.00 - ETA: 22:34 - ETA: 7:00 - loss: 0.1320 - acc: 0.9503 - tau_crossentropy: 0.1366 - Le: 0.0433 - Lmu: 0.0055 - Ljet: 0.0878 - sLe: 0.0433 - sLmu: 0.0055 - sLjet: 0.0878 - weighted_acc: 0.9499 - weighted_tau_crossentropy: 0.1354 - weighted_Le: 0.0558 - weighted_Lmu: 0.0066 - weighted_Ljet: 0.0730 - weighted_sLe: 0.0558 - weighted_sLmu: 0.0066 - weighted_sLjet: 0. - ETA: 6:59 - loss: 0.1320 - acc: 0.9503 - tau_crossentropy: 0.136 - ETA: 4:2 - ETA: 2:40 - loss: 0.1322 - acc: 0.9502 - tau_crossentropy: 0.1368 - Le: 0.0434 - Lmu: 0.0055 - Ljet: 0.0879 - sLe: 0.0434 - sLmu: 0.0055 - sLjet: 0.0879 - w - ETA: 2:10 - loss: 0.1323 - acc: 0.9501 - tau_crossentropy: 0.1368 - Le: 0.0434 - Lmu: 0.0055 - Ljet: 0.0879 - sLe - ETA: 22s - loss: 0.1323 - acc: 0.9501 - tau_crossentropy: 0.1369 - Le: 0.0434 - Lmu: 0.0055 - Ljet: 0.0880 - sLe: 0.0434 -\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95009, saving model to DeepTau2017v2p3_step1_acc.hdf5\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.94984, saving model to DeepTau2017v2p3_step1_acc.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13229, saving model to DeepTau2017v2p3_step1_loss.hdf5\n",
      "\n",
      "Epoch 00001: val_tau_crossentropy improved from inf to 0.13692, saving model to DeepTau2017v2p3_step1_tau_crossentropy.hdf5\n",
      "\n",
      "Epoch 00001: val_Le improved from inf to 0.04344, saving model to DeepTau2017v2p3_step1_Le.hdf5\n",
      "\n",
      "Epoch 00001: val_Lmu improved from inf to 0.00550, saving model to DeepTau2017v2p3_step1_Lmu.hdf5\n",
      "\n",
      "Epoch 00001: val_Ljet improved from inf to 0.08799, saving model to DeepTau2017v2p3_step1_Ljet.hdf5\n",
      "\n",
      "Epoch 00001: val_weighted_tau_crossentropy improved from inf to 0.13543, saving model to DeepTau2017v2p3_step1_weighted_tau_crossentropy.hdf5\n",
      "\n",
      "Epoch 00001: val_weighted_Le improved from inf to 0.05572, saving model to DeepTau2017v2p3_step1_weighted_Le.hdf5\n",
      "\n",
      "Epoch 00001: val_weighted_Lmu improved from inf to 0.00655, saving model to DeepTau2017v2p3_step1_weighted_Lmu.hdf5\n",
      "\n",
      "Epoch 00001: val_weighted_Ljet improved from inf to 0.07316, saving model to DeepTau2017v2p3_step1_weighted_Ljet.hdf5\n",
      "331959/331959 [==============================] - 178390s 537ms/step - loss: 0.2219 - acc: 0.8249 - tau_crossentropy: 0.2164 - Le: 0.0727 - Lmu: 0.0184 - Ljet: 0.1254 - sLe: 0.0727 - sLmu: 0.0184 - sLjet: 0.1254 - weighted_acc: 0.8180 - weighted_tau_crossentropy: 0.2284 - weighted_Le: 0.0899 - weighted_Lmu: 0.0279 - weighted_Ljet: 0.1106 - weighted_sLe: 0.0899 - weighted_sLmu: 0.0279 - weighted_sLjet: 0.1106 - val_loss: 0.1323 - val_acc: 0.9501 - val_tau_crossentropy: 0.1369 - val_Le: 0.0434 - val_Lmu: 0.0055 - val_Ljet: 0.0880 - val_sLe: 0.0434 - val_sLmu: 0.0055 - val_sLjet: 0.0880 - val_weighted_acc: 0.9498 - val_weighted_tau_crossentropy: 0.1354 - val_weighted_Le: 0.0557 - val_weighted_Lmu: 0.0065 - val_weighted_Ljet: 0.0732 - val_weighted_sLe: 0.0557 - val_weighted_sLmu: 0.0065 - val_weighted_sLjet: 0.0732\n",
      "Epoch 2/2\n",
      "30500/30500 [==============================] - 4627s 152ms/step - loss: 0.1256 - acc: 0.9525 - tau_crossentropy: 0.1312 - Le: 0.0409 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0409 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9530 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0531 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0682 - weighted_sLe: 0.0531 - weighted_sLmu: 0.0060 - weighted_sLjet: 0.068214:11 - loss: 0.1248 - acc: 0.9529 - tau_crossentropy: 0.1305 - Le: 0.0406 - Lmu: 0.0053 - Ljet: 0.0846 - sLe: 0.0406 - sLmu: 0.0053 - sLjet: 0.0846 - weighted_acc: 0.9540 - weighted_tau_crossentropy: 0.1259 - weighted_Le: 0.0531 - weight - ETA: 14:02 - loss: 0.1248 - acc: 0.9529 - tau_crossentropy: 0.1305 -  - ETA: 13:41 - loss: 0.1249 - acc: 0.9529 - tau_crossentropy: 0.1306 - Le: 0.0407 - Lmu: 0.0053 - Ljet: 0.0846 - sLe: 0.0407 - sLmu: 0.0053 - sLjet: 0.0846 - weighted_acc: 0.9538 - weighted_tau_crossentropy: 0.1264 - weighted_Le: 0.0531 - weighted_Lmu: 0.0061 - weighted_Ljet: 0.0672 - weighted_sLe: 0.0531 - weighted_sLmu: 0.0061 - weighted_sLj - ETA: 13:40 - loss: 0.1249 - acc: 0.9529 - tau_crossentropy: 0.1306 - Le: 0.0407 - Lmu: 0.0053 - Ljet: 0.0846 - sLe: 0.0407 - sLmu: 0.0053 - sLjet: 0.0 - ETA: 9:37 - loss: 0.1252 - acc: 0.9528 - tau_crossentropy: 0.1307 - Le: 0.0407 - Lmu: 0.0053 - Ljet: 0.0847 - sLe: 0.0407 - sLmu: 0.0053 - sLjet: 0.0847 - weighted_acc: 0.9534 - weighted_tau_crossentropy: 0.1270 - weighted_Le: 0.0533 - weighted_Lmu: 0. - ETA: 9:22 - loss: 0.1253 - acc: 0.9528 - tau_crossentropy: 0.1307 - Le: 0.0407 - Lmu: 0.0053 - Ljet: 0.0848 - sLe: 0.0407 - sLmu: 0.0053 - sLjet: 0.0848 - weighted_acc: 0.9532 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0533 - weighted_Lmu: 0.0061 - weighted_Ljet: 0.0679 - weighted_sLe: 0.0533 - weighted_sLmu: 0.0061 - weighted_sLjet: 0.06 - ETA: 9:22 - loss: 0.1253 - acc: 0.9528 - tau_crossentropy: 0.1307 - Le: 0.0407 - Lmu: 0.0053 - Ljet: 0.0848 - sLe: 0.0407 - sLmu: 0.0053 - sLjet: 0.0848 - weighted_acc: 0.9532 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0533 - weighted_Lmu: 0.0061 - weighted_Ljet: 0.0679 - weighted_sLe: 0.0533 - - ETA: 9:15 - loss: 0.1253 - acc: 0.9528 - tau_crossentropy: 0.1307 - Le: 0.0407 - Lmu: 0.0053 - Ljet: 0.0848 - sLe: 0.0407 - sLmu: 0.0053 - sLjet: 0.0848 - weighted_acc: 0.9532 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0533 - weighted_Lmu: 0.0061 - weighted_Ljet: 0.0679 - weighted_sLe: 0.0533 - weighted_sLmu: 0.0061 - weighted_sLjet: 0. - ETA: 9:14 - loss: 0.1253 - acc: 0.9528 - tau_crossentropy: 0.1307 - Le:  - ETA: 4:56 - loss: 0.1254 - acc: 0.9527 - tau_crossentropy: 0.1310 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0849 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0849 - weighted_acc: 0.9532 - weighted_tau_crossentropy: 0.1272 - weighted_Le: 0.0531 - weighted_Lmu: 0.0 - ETA: 4:41 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1310 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0849 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0849 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - w - ETA: 4:27 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1310 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0849 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0849 - weighted_acc: 0.9531 - weighted_tau_crossent - ETA: 4:04 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0849 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0849 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0533 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0680 - weighted_sLe: 0.0533 - weighted_sLmu: 0.0060 - weighted_sLjet: 0 - ETA: 4:03 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0849 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0849 - weighted_acc: 0.9531 - weighted_tau_crossentr - ETA: 3:40 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0533 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0680 - weighted_sLe: 0.0533 - - ETA: 3:33 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0680 - weighted_sLe: 0.0532 - - ETA: 3:26 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0 - ETA: 3:11 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0680 - weighted_sLe: 0.0532 - weighted_sLmu: 0.0060 - weighted_sLjet: 0.068 - ETA: 3:10 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0680 - weighted_sLe: 0.0532 - weighted_sLmu: 0.0060 - weighted_sLjet: 0. - ETA: 3:10 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0680 - weighted_sLe: 0.0532 -  - ETA: 3:03 - loss: 0.1255 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1272 - weighted_Le: 0.0532 - weighted_Lmu - ETA: 2:47 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0. - ETA: 2:25 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_cro - ETA: 2:01 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0409 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0409 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0681 - weighted_sLe: 0.0532 - we - ETA: 1:54 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0408 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9531 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0532 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0681 - weighted_sLe: 0.0532  - ETA: 1:47 - loss: 0.1256 - acc: 0.9526 - tau_crossentropy: 0.1311 - Le: 0.0408 - Lmu: 0.0053 - Ljet: 0 - ETA: 1:09 - loss: 0.1257 - acc: 0.9526 - tau_crossentropy: 0.1312 - Le: 0.0409 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0409 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9530 - weighted_tau_crossentropy: 0.1275 - we - ETA: 54s - loss: 0.1257 - acc: 0.9525 - tau_crossen - ETA: - ETA: 0s - loss: 0.1256 - acc: 0.9525 - tau_crossentropy: 0.1312 - Le: 0.0409 - Lmu: 0.0053 - Ljet: 0.0850 - sLe: 0.0409 - sLmu: 0.0053 - sLjet: 0.0850 - weighted_acc: 0.9530 - weighted_tau_crossentropy: 0.1273 - weighted_Le: 0.0531 - weighted_Lmu: 0.0060 - weighted_Ljet: 0.0682 - weighted_sLe: 0.0531 - weighted_sLmu: 0.0060 - weighted_sLjet: \n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95009 to 0.95253, saving model to DeepTau2017v2p3_step1_acc.hdf5\n",
      "\n",
      "Epoch 00002: val_weighted_acc improved from 0.94984 to 0.95304, saving model to DeepTau2017v2p3_step1_acc.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13229 to 0.12560, saving model to DeepTau2017v2p3_step1_loss.hdf5\n",
      "\n",
      "Epoch 00002: val_tau_crossentropy improved from 0.13692 to 0.13123, saving model to DeepTau2017v2p3_step1_tau_crossentropy.hdf5\n",
      "\n",
      "Epoch 00002: val_Le improved from 0.04344 to 0.04089, saving model to DeepTau2017v2p3_step1_Le.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_Lmu improved from 0.00550 to 0.00531, saving model to DeepTau2017v2p3_step1_Lmu.hdf5\n",
      "\n",
      "Epoch 00002: val_Ljet improved from 0.08799 to 0.08504, saving model to DeepTau2017v2p3_step1_Ljet.hdf5\n",
      "\n",
      "Epoch 00002: val_weighted_tau_crossentropy improved from 0.13543 to 0.12729, saving model to DeepTau2017v2p3_step1_weighted_tau_crossentropy.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\python36_64\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\python36_64\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../../python\\DataLoader.py\", line 41, in LoaderThread\n",
      "  File \"../../python\\DataLoader.py\", line 17, in read_hdf\n",
      "    read_hdf_lock.release()\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 389, in read_hdf\n",
      "    return store.select(key, auto_close=auto_close, **kwargs)\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 740, in select\n",
      "    return it.get_result()\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 1518, in get_result\n",
      "    results = self.func(self.start, self.stop, where)\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 733, in func\n",
      "    columns=columns)\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 4254, in read\n",
      "    if not self.read_axes(where=where, **kwargs):\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 3444, in read_axes\n",
      "    values = self.selection.select()\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\pytables.py\", line 4853, in select\n",
      "    return self.table.table.read(start=self.start, stop=self.stop)\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tables\\table.py\", line 1934, in read\n",
      "    arr = self._read(start, stop, step, field, out)\n",
      "  File \"C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tables\\table.py\", line 1848, in _read\n",
      "    self._read_records(start, stop - start, result)\n",
      "  File \"tables\\tableextension.pyx\", line 574, in tables.tableextension.Table._read_records\n",
      "tables.exceptions.HDF5ExtError: Problems reading records.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_weighted_Le improved from 0.05572 to 0.05308, saving model to DeepTau2017v2p3_step1_weighted_Le.hdf5\n",
      "\n",
      "Epoch 00002: val_weighted_Lmu improved from 0.00655 to 0.00599, saving model to DeepTau2017v2p3_step1_weighted_Lmu.hdf5\n",
      "\n",
      "Epoch 00002: val_weighted_Ljet improved from 0.07316 to 0.06823, saving model to DeepTau2017v2p3_step1_weighted_Ljet.hdf5\n",
      "331959/331959 [==============================] - 176858s 533ms/step - loss: 0.1807 - acc: 0.9190 - tau_crossentropy: 0.1780 - Le: 0.0586 - Lmu: 0.0104 - Ljet: 0.1090 - sLe: 0.0586 - sLmu: 0.0104 - sLjet: 0.1090 - weighted_acc: 0.9108 - weighted_tau_crossentropy: 0.1867 - weighted_Le: 0.0765 - weighted_Lmu: 0.0168 - weighted_Ljet: 0.0934 - weighted_sLe: 0.0765 - weighted_sLmu: 0.0168 - weighted_sLjet: 0.0934 - val_loss: 0.1256 - val_acc: 0.9525 - val_tau_crossentropy: 0.1312 - val_Le: 0.0409 - val_Lmu: 0.0053 - val_Ljet: 0.0850 - val_sLe: 0.0409 - val_sLmu: 0.0053 - val_sLjet: 0.0850 - val_weighted_acc: 0.9530 - val_weighted_tau_crossentropy: 0.1273 - val_weighted_Le: 0.0531 - val_weighted_Lmu: 0.0060 - val_weighted_Ljet: 0.0682 - val_weighted_sLe: 0.0531 - val_weighted_sLmu: 0.0060 - val_weighted_sLjet: 0.0682\n"
     ]
    }
   ],
   "source": [
    "fit_hist = run_training('step{}'.format(1), model_name, loader, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
