{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, AlphaDropout, Activation, BatchNormalization, Flatten, \\\n",
    "                                    Concatenate, PReLU, TimeDistributed, LSTM, Masking\n",
    "from keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "sys.path.insert(0, \"../../python\")\n",
    "from common import *\n",
    "from DataLoader import DataLoader, read_hdf_lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDense(Dense):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(MaskedDense, self).__init__(units, **kwargs)\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        base_out = super(MaskedDense, self).call(inputs)\n",
    "        if mask is None:\n",
    "            return base_out\n",
    "        zeros = tf.zeros_like(base_out)\n",
    "        return tf.where(mask, base_out, zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, filepath, **kwargs):\n",
    "        super(SafeModelCheckpoint, self).__init__(filepath, **kwargs)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        read_hdf_lock.acquire()\n",
    "        super(SafeModelCheckpoint, self).on_epoch_end(epoch, logs)\n",
    "        read_hdf_lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSetup:\n",
    "    def __init__(self, activation, activation_shared_axes, dropout_rate, first_layer_size, last_layer_size, decay_factor,\n",
    "                 kernel_regularizer, time_distributed):\n",
    "        self.activation = activation\n",
    "        self.activation_shared_axes = activation_shared_axes\n",
    "        if activation == 'relu' or activation == 'PReLU' or activation == 'tanh':\n",
    "            self.DropoutType = Dropout\n",
    "            self.kernel_init = 'he_uniform'\n",
    "            self.apply_batch_norm = True\n",
    "        elif activation == 'selu':\n",
    "            self.DropoutType = AlphaDropout\n",
    "            self.kernel_init = 'lecun_normal'\n",
    "            self.apply_batch_norm = False\n",
    "        else:\n",
    "            raise RuntimeError('Activation \"{}\" not supported.'.format(activation))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.first_layer_size = first_layer_size\n",
    "        self.last_layer_size = last_layer_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.time_distributed = time_distributed\n",
    "    \n",
    "    def RecalcLayerSizes(self, n_input_features, width_factor, compression_factor):\n",
    "        self.first_layer_size = int(math.ceil(n_input_features * (1 + self.dropout_rate) * width_factor))\n",
    "        self.last_layer_size = int(math.ceil(n_input_features * (1 + self.dropout_rate) * compression_factor))\n",
    "    \n",
    "def add_block_ending(net_setup, name_format, layer):\n",
    "    if net_setup.apply_batch_norm:\n",
    "        norm_layer = BatchNormalization(name=name_format.format('norm'))\n",
    "        if net_setup.time_distributed:\n",
    "            norm_layer = TimeDistributed(norm_layer, name=name_format.format('norm'))\n",
    "        norm_layer = norm_layer(layer)\n",
    "    else:\n",
    "        norm_layer = layer\n",
    "    if net_setup.activation == 'PReLU':\n",
    "        activation_layer = PReLU(shared_axes=net_setup.activation_shared_axes,\n",
    "                                 name=name_format.format('activation'))(norm_layer)\n",
    "    else:\n",
    "        activation_layer = Activation(net_setup.activation, name=name_format.format('activation'))(norm_layer)\n",
    "    if net_setup.dropout_rate > 0:\n",
    "        return net_setup.DropoutType(net_setup.dropout_rate, name=name_format.format('dropout'))(activation_layer)\n",
    "    return activation_layer\n",
    "\n",
    "def dense_block(prev_layer, kernel_size, net_setup, block_name, n):\n",
    "    DenseType = MaskedDense if net_setup.time_distributed else Dense\n",
    "    dense = DenseType(kernel_size, name=\"{}_dense_{}\".format(block_name, n),\n",
    "                      kernel_initializer=net_setup.kernel_init,\n",
    "                      kernel_regularizer=net_setup.kernel_regularizer)\n",
    "    if net_setup.time_distributed:\n",
    "        dense = TimeDistributed(dense, name=\"{}_dense_{}\".format(block_name, n))\n",
    "    dense = dense(prev_layer)\n",
    "    return add_block_ending(net_setup, '{}_{{}}_{}'.format(block_name, n), dense)\n",
    "\n",
    "def reduce_n_features_1d(input_layer, net_setup, block_name):\n",
    "    prev_layer = input_layer\n",
    "    current_size = net_setup.first_layer_size\n",
    "    n = 1\n",
    "    while True:\n",
    "        prev_layer = dense_block(prev_layer, current_size, net_setup, block_name, n)\n",
    "        if current_size == net_setup.last_layer_size: break\n",
    "        current_size = max(net_setup.last_layer_size, int(current_size / net_setup.decay_factor))\n",
    "        n += 1\n",
    "    return prev_layer\n",
    "\n",
    "def dense_block_sequence(input_layer, net_setup, n_layers, block_name):\n",
    "    prev_layer = input_layer\n",
    "    current_size = net_setup.first_layer_size\n",
    "    for n in range(n_layers):\n",
    "        prev_layer = dense_block(prev_layer, current_size, net_setup, block_name, n+1)\n",
    "    return prev_layer\n",
    "\n",
    "\n",
    "def create_model(net_config):\n",
    "    tau_net_setup = NetSetup('PReLU', None, 0.2, 128, 128, 1.4, None, False)\n",
    "    comp_net_setup = NetSetup('PReLU', [1], 0.2, 1024, 64, 1.4, None, True)\n",
    "    #dense_net_setup = NetSetup('relu', 0, 512, 32, 1.4, keras.regularizers.l1(1e-5))\n",
    "    dense_net_setup = NetSetup('PReLU', None, 0.2, 512, 64, 1.4, None, False)\n",
    "            \n",
    "    input_layers = []\n",
    "    high_level_features = []\n",
    "\n",
    "    if len(net_config.tau_branches) > 0:\n",
    "        input_layer_tau = Input(name=\"input_tau\", shape=(len(net_config.tau_branches),))\n",
    "        input_layers.append(input_layer_tau)\n",
    "        tau_net_setup.RecalcLayerSizes(len(net_config.tau_branches), 2, 2)\n",
    "        processed_tau = dense_block_sequence(input_layer_tau, tau_net_setup, 4, 'tau')\n",
    "        high_level_features.append(processed_tau)\n",
    "    \n",
    "    for loc in net_config.cell_locations:\n",
    "        reduced_inputs = []\n",
    "        for comp_id in range(len(net_config.comp_names)):\n",
    "            comp_name = net_config.comp_names[comp_id]\n",
    "            n_comp_features = len(input_cell_external_branches) + len(net_config.comp_branches[comp_id])\n",
    "            input_layer_comp = Input(name=\"input_{}_{}\".format(loc, comp_name),\n",
    "                                     shape=(n_cells_eta[loc] * n_cells_phi[loc], n_comp_features))\n",
    "            input_layers.append(input_layer_comp)\n",
    "            comp_net_setup.RecalcLayerSizes(n_comp_features, 2, 2)\n",
    "            input_layer_comp_masked = Masking(name=\"input_{}_{}_masking\".format(loc, comp_name))(input_layer_comp)\n",
    "            reduced_comp = dense_block_sequence(input_layer_comp_masked, comp_net_setup, 4, \"{}_{}\".format(loc, comp_name))\n",
    "            reduced_inputs.append(reduced_comp)\n",
    "            \n",
    "        cell_output_size = 32\n",
    "        if len(net_config.comp_names) > 1:\n",
    "            cell_input = Concatenate(name=\"{}_cell_concat_0\".format(loc), axis=2)(reduced_inputs)\n",
    "        else:\n",
    "            cell_input = reduced_inputs[0]\n",
    "        \n",
    "        cell_lstm_1 = LSTM(units=cell_output_size, return_sequences=True, name='{}_lstm_1'.format(loc))(cell_input)\n",
    "        cell_lstm_1_drop = Dropout(comp_net_setup.dropout_rate, name='{}_lstm_1_dropout'.format(loc))(cell_lstm_1)\n",
    "        \n",
    "        cell_input_1 = Concatenate(name=\"{}_cell_concat_1\".format(loc), axis=2)([cell_input, cell_lstm_1_drop])\n",
    "        cell_lstm_2 = LSTM(units=cell_output_size, return_sequences=True, name='{}_lstm_2'.format(loc))(cell_input_1)\n",
    "        cell_lstm_2_drop = Dropout(comp_net_setup.dropout_rate, name='{}_lstm_2_dropout'.format(loc))(cell_lstm_2)\n",
    "\n",
    "        cell_input_2 = Concatenate(name=\"{}_cell_concat_2\".format(loc), axis=2)([cell_input, cell_lstm_2_drop])\n",
    "        cell_lstm_3 = LSTM(units=cell_output_size, return_sequences=False, name='{}_lstm_3'.format(loc))(cell_input_2)\n",
    "        cell_lstm_3_drop = Dropout(comp_net_setup.dropout_rate, name='{}_lstm_3_dropout'.format(loc))(cell_lstm_3)\n",
    "\n",
    "        high_level_features.append(cell_lstm_3_drop)\n",
    "        \n",
    "    if len(high_level_features) > 1:\n",
    "        features_concat = Concatenate(name=\"features_concat\")(high_level_features)\n",
    "    else:\n",
    "        features_concat = high_level_features[0]\n",
    "    if net_config.final:\n",
    "        dense_net_setup.first_layer_size = features_concat.shape[1]\n",
    "        final_dense = dense_block_sequence(features_concat, dense_net_setup, 4, 'final')\n",
    "        output_layer = Dense(n_outputs, name=\"final_dense_last\",\n",
    "                     kernel_initializer=dense_net_setup.kernel_init)(final_dense)\n",
    "\n",
    "    else:\n",
    "        final_dense = dense_block(features_concat, 1024, dense_net_setup,\n",
    "                                  'tmp_{}'.format(net_config.name), 1)\n",
    "        output_layer = Dense(n_outputs, name=\"tmp_{}_dense_last\".format(net_config.name),\n",
    "                             kernel_initializer=dense_net_setup.kernel_init)(final_dense)\n",
    "    softmax_output = Activation(\"softmax\", name=\"main_output\")(output_layer)\n",
    "    \n",
    "    model = Model(input_layers, softmax_output, name=\"DeepTau2017v2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate):\n",
    "    opt = keras.optimizers.Adam(lr=learning_rate)\n",
    "    #opt = keras.optimizers.Nadam(lr=learning_rate)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    metrics = [ \"accuracy\", TauLosses.tau_crossentropy, TauLosses.tau_crossentropy_v2, TauLosses.Le, TauLosses.Lmu,\n",
    "               TauLosses.Ljet, TauLosses.He, TauLosses.Hmu, TauLosses.Htau, TauLosses.Hjet ]\n",
    "    model.compile(loss=TauLosses.tau_crossentropy_v2, optimizer=opt, metrics=metrics, weighted_metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_tau (InputLayer)       (None, 47)                0         \n",
      "_________________________________________________________________\n",
      "tau_dense_1 (Dense)          (None, 113)               5424      \n",
      "_________________________________________________________________\n",
      "tau_norm_1 (BatchNormalizati (None, 113)               452       \n",
      "_________________________________________________________________\n",
      "tau_activation_1 (PReLU)     (None, 113)               113       \n",
      "_________________________________________________________________\n",
      "tau_dropout_1 (Dropout)      (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "tau_dense_2 (Dense)          (None, 113)               12882     \n",
      "_________________________________________________________________\n",
      "tau_norm_2 (BatchNormalizati (None, 113)               452       \n",
      "_________________________________________________________________\n",
      "tau_activation_2 (PReLU)     (None, 113)               113       \n",
      "_________________________________________________________________\n",
      "tau_dropout_2 (Dropout)      (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "tau_dense_3 (Dense)          (None, 113)               12882     \n",
      "_________________________________________________________________\n",
      "tau_norm_3 (BatchNormalizati (None, 113)               452       \n",
      "_________________________________________________________________\n",
      "tau_activation_3 (PReLU)     (None, 113)               113       \n",
      "_________________________________________________________________\n",
      "tau_dropout_3 (Dropout)      (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "tau_dense_4 (Dense)          (None, 113)               12882     \n",
      "_________________________________________________________________\n",
      "tau_norm_4 (BatchNormalizati (None, 113)               452       \n",
      "_________________________________________________________________\n",
      "tau_activation_4 (PReLU)     (None, 113)               113       \n",
      "_________________________________________________________________\n",
      "tau_dropout_4 (Dropout)      (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "tmp_preTau_dense_1 (Dense)   (None, 1024)              116736    \n",
      "_________________________________________________________________\n",
      "tmp_preTau_norm_1 (BatchNorm (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "tmp_preTau_activation_1 (PRe (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "tmp_preTau_dropout_1 (Dropou (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "tmp_preTau_dense_last (Dense (None, 4)                 4100      \n",
      "_________________________________________________________________\n",
      "main_output (Activation)     (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 172,286\n",
      "Trainable params: 169,334\n",
      "Non-trainable params: 2,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "TauLosses.SetSFs(1, 1, 3, 1)\n",
    "model_name = \"DeepTau2017v2p4_preTau\"\n",
    "model = create_model(netConf_preTau)\n",
    "compile_model(model, 1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45426"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetNumberOfTrainableParams(model, exclude_prefix = 'tmp'):\n",
    "    count = 0\n",
    "    for w in model.trainable_weights:\n",
    "        if not w.name.startswith(exclude_prefix):\n",
    "            count += K.count_params(w)\n",
    "    return count\n",
    "GetNumberOfTrainableParams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_file(f_name):\n",
    "    file_objs = [ obj for obj in gc.get_objects() if (\"TextIOWrapper\" in str(type(obj))) and (obj.name == f_name)]\n",
    "    for obj in file_objs:\n",
    "        obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCheckpoint(Callback):\n",
    "    def __init__(self, time_interval, file_name_prefix):\n",
    "        self.time_interval = time_interval\n",
    "        self.file_name_prefix = file_name_prefix\n",
    "        self.initial_time = time.time()\n",
    "        self.last_check_time = self.initial_time\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 100 != 0: return\n",
    "        current_time = time.time()\n",
    "        delta_t = current_time - self.last_check_time\n",
    "        if delta_t >= self.time_interval:\n",
    "            abs_delta_t_h = (current_time - self.initial_time) / 60. / 60.\n",
    "            read_hdf_lock.acquire()\n",
    "            self.model.save('{}_b{}_{:.1f}h.h5'.format(self.file_name_prefix, batch, abs_delta_t_h))\n",
    "            read_hdf_lock.release()\n",
    "            self.last_check_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_suffix, model_name, data_loader, epoch, n_epochs):\n",
    "\n",
    "    train_name = '%s_%s' % (model_name, train_suffix)\n",
    "    \n",
    "    cb_acc = []\n",
    "    for acc_name in [\"acc\", \"weighted_acc\"]:\n",
    "        cb_acc.append(SafeModelCheckpoint(\"%s_acc.hdf5\" % train_name, monitor=\"val_%s\" % acc_name, save_best_only=True,\n",
    "                                          save_weights_only=False, mode=\"max\", verbose=1))\n",
    "    \n",
    "    losses_names = [ \"loss\" ]\n",
    "    for w_suffix in [ \"\", \"weighted_\" ]:\n",
    "        for l_name in [ \"tau_crossentropy\", \"tau_crossentropy_v2\", \"Le\", \"Lmu\", \"Ljet\", \"He\", \"Hmu\", \"Htau\", \"Hjet\" ]:\n",
    "            losses_names.append(w_suffix + l_name)\n",
    "    cb_losses = []\n",
    "    for loss_name in losses_names:\n",
    "        cb_losses.append(ModelCheckpoint(\"%s_%s.hdf5\" % (train_name, loss_name), monitor=\"val_%s\" % loss_name,\n",
    "                                         save_best_only=True, save_weights_only=False, mode=\"min\", verbose=1))\n",
    "\n",
    "    log_name = \"%s.log\" % train_name\n",
    "    if os.path.isfile(log_name):\n",
    "        close_file(log_name)\n",
    "        os.remove(log_name)\n",
    "    csv_log = CSVLogger(log_name, append=True)\n",
    "\n",
    "    time_checkpoint = TimeCheckpoint(4*60*60, '{}_historic'.format(train_name))\n",
    "    pbar = TQDMNotebookCallback(leave_outer=True, show_outer=True, leave_inner = True)\n",
    "    callbacks = [pbar, time_checkpoint, csv_log, *cb_acc, *cb_losses]\n",
    "    fit_hist = model.fit_generator(data_loader.generator(True), validation_data=data_loader.generator(False),\n",
    "                                   steps_per_epoch=data_loader.steps_per_epoch, validation_steps=data_loader.validation_steps,\n",
    "                                   callbacks=callbacks, epochs=n_epochs, initial_epoch=epoch, verbose=0)\n",
    "\n",
    "    model.save(\"%s_final.hdf5\" % train_name)\n",
    "    return fit_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72491602 66391602 6100000\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('N:/tau-ml/tuples-v2-t3/training/part_*.h5', netConf_preTau, 10000, 1000000, validation_size=6100000,\n",
    "                    max_queue_size=200, n_passes=-1, return_grid=False)\n",
    "\n",
    "print(loader.total_size, loader.data_size, loader.validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccacca328a114a25ae9c8186b6bde99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=2, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84955408e46f4e3f9c2a079fed48abd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=6640, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87661, saving model to DeepTau2017v2p4_preTau_step1_acc.hdf5\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.86736, saving model to DeepTau2017v2p4_preTau_step1_acc.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37294, saving model to DeepTau2017v2p4_preTau_step1_loss.hdf5\n",
      "\n",
      "Epoch 00001: val_tau_crossentropy improved from inf to 0.22055, saving model to DeepTau2017v2p4_preTau_step1_tau_crossentropy.hdf5\n",
      "\n",
      "Epoch 00001: val_tau_crossentropy_v2 improved from inf to 0.34176, saving model to DeepTau2017v2p4_preTau_step1_tau_crossentropy_v2.hdf5\n",
      "\n",
      "Epoch 00001: val_Le improved from inf to 0.08699, saving model to DeepTau2017v2p4_preTau_step1_Le.hdf5\n",
      "\n",
      "Epoch 00001: val_Lmu improved from inf to 0.08879, saving model to DeepTau2017v2p4_preTau_step1_Lmu.hdf5\n",
      "\n",
      "Epoch 00001: val_Ljet improved from inf to 0.15505, saving model to DeepTau2017v2p4_preTau_step1_Ljet.hdf5\n",
      "\n",
      "Epoch 00001: val_He improved from inf to 0.09288, saving model to DeepTau2017v2p4_preTau_step1_He.hdf5\n",
      "\n",
      "Epoch 00001: val_Hmu improved from inf to 0.09193, saving model to DeepTau2017v2p4_preTau_step1_Hmu.hdf5\n",
      "\n",
      "Epoch 00001: val_Htau improved from inf to 0.07415, saving model to DeepTau2017v2p4_preTau_step1_Htau.hdf5\n",
      "\n",
      "Epoch 00001: val_Hjet improved from inf to 0.18149, saving model to DeepTau2017v2p4_preTau_step1_Hjet.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedtau_crossentropy available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedtau_crossentropy_v2 available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedLe available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedLmu available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedLjet available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedHe available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedHmu available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedHtau available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_weightedHjet available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbcefd62be04323a6058c56e7e53804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=6640, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_acc improved from 0.87661 to 0.87697, saving model to DeepTau2017v2p4_preTau_step1_acc.hdf5\n",
      "\n",
      "Epoch 00002: val_weighted_acc improved from 0.86736 to 0.87012, saving model to DeepTau2017v2p4_preTau_step1_acc.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37294 to 0.32965, saving model to DeepTau2017v2p4_preTau_step1_loss.hdf5\n",
      "\n",
      "Epoch 00002: val_tau_crossentropy improved from 0.22055 to 0.21342, saving model to DeepTau2017v2p4_preTau_step1_tau_crossentropy.hdf5\n",
      "\n",
      "Epoch 00002: val_tau_crossentropy_v2 improved from 0.34176 to 0.30920, saving model to DeepTau2017v2p4_preTau_step1_tau_crossentropy_v2.hdf5\n",
      "\n",
      "Epoch 00002: val_Le did not improve from 0.08699\n",
      "\n",
      "Epoch 00002: val_Lmu improved from 0.08879 to 0.08146, saving model to DeepTau2017v2p4_preTau_step1_Lmu.hdf5\n",
      "\n",
      "Epoch 00002: val_Ljet improved from 0.15505 to 0.14485, saving model to DeepTau2017v2p4_preTau_step1_Ljet.hdf5\n",
      "\n",
      "Epoch 00002: val_He did not improve from 0.09288\n",
      "\n",
      "Epoch 00002: val_Hmu did not improve from 0.09193\n",
      "\n",
      "Epoch 00002: val_Htau improved from 0.07415 to 0.05571, saving model to DeepTau2017v2p4_preTau_step1_Htau.hdf5\n",
      "\n",
      "Epoch 00002: val_Hjet improved from 0.18149 to 0.16164, saving model to DeepTau2017v2p4_preTau_step1_Hjet.hdf5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_hist = run_training('step{}'.format(1), model_name, loader, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
